{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94497,"databundleVersionId":11231232,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T04:14:55.485179Z","iopub.execute_input":"2025-02-27T04:14:55.485546Z","iopub.status.idle":"2025-02-27T04:14:56.674624Z","shell.execute_reply.started":"2025-02-27T04:14:55.485507Z","shell.execute_reply":"2025-02-27T04:14:56.673587Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/param-tunint-iot-b-2/sample_submission.csv\n/kaggle/input/param-tunint-iot-b-2/train.csv\n/kaggle/input/param-tunint-iot-b-2/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.compose import ColumnTransformer\n\n# Load datasets\ntrain_df = pd.read_csv(\"/kaggle/input/param-tunint-iot-b-2/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/param-tunint-iot-b-2/test.csv\")\n\n# Drop 'id' and 'outcome' from train data\nX = train_df.drop(columns=['id', 'outcome'])\ny = train_df['outcome']\n\n# Drop 'id' from test data\nX_test = test_df.drop(columns=['id'])\n\n# Identify categorical and numerical columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\nnumerical_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n\n# OneHotEncode categorical columns and standardize numerical features\npreprocessor = ColumnTransformer([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n    ('scaler', StandardScaler(), numerical_cols)\n])\n\n# Transform features\nX = preprocessor.fit_transform(X)\nX_test = preprocessor.transform(X_test)\n\n# Encode target variable\ny_encoded = pd.factorize(y)[0]\n\n# Splitting data\nX_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n\n# Model training and evaluation\nmodels = {\n    \"RandomForest\": RandomForestClassifier(),\n    \"GradientBoosting\": GradientBoostingClassifier(),\n    \"LogisticRegression\": LogisticRegression(max_iter=500),\n    \"XGBClassifier\": XGBClassifier()\n}\n\nbest_model = None\nbest_f1 = 0\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    f1 = f1_score(y_val, y_pred, average='micro')\n    print(f\"{name} F1-Score: {f1:.4f}\")\n    if f1 > best_f1:\n        best_f1 = f1\n        best_model = model\n\n# Hyperparameter tuning using GridSearchCV\nparam_grids = {\n    \"RandomForestClassifier\": {\n        \"n_estimators\": [100, 200],\n        \"max_depth\": [None, 10],\n        \"criterion\": [\"gini\", \"entropy\"],\n        \"min_samples_split\": [2, 5],\n        \"min_samples_leaf\": [1, 2]\n    },\n    \"GradientBoostingClassifier\": {\n        \"n_estimators\": [100, 200],\n        \"learning_rate\": [0.05, 0.1],\n        \"max_depth\": [3, 5],\n        \"subsample\": [0.8, 1.0]\n    },\n    \"LogisticRegression\": {\n        \"C\": [0.1, 1, 10],\n        \"solver\": [\"liblinear\", \"lbfgs\"]\n    },\n    \"XGBClassifier\": {\n        \"n_estimators\": [100, 200],\n        \"learning_rate\": [0.05, 0.1],\n        \"max_depth\": [3, 5],\n        \"subsample\": [0.8, 1.0],\n        \"colsample_bytree\": [0.8, 1.0]\n    }\n}\n\nif best_model:\n    model_name = best_model.__class__.__name__\n    if model_name in param_grids:\n        print(f\"Tuning {model_name} with GridSearchCV...\")\n        grid_search = GridSearchCV(best_model, param_grids[model_name], cv=5, scoring='f1_micro', n_jobs=-1)\n        grid_search.fit(X_train, y_train)\n        best_params = grid_search.best_params_\n        \n        print(f\"Best {model_name} parameters: {best_params}\")\n        print(f\"Best {model_name} F1-Score after tuning: {grid_search.best_score_:.4f}\\n\")\n        \n        # Train the best model with full data\n        best_model.set_params(**best_params)\n        best_model.fit(X, y_encoded)\n\n# Prediction on test data\ntest_predictions = best_model.predict(X_test)\n\n# Convert numerical labels back to original labels\ntest_predictions = pd.Series(test_predictions).map(dict(enumerate(y.unique())))\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"], \"outcome\": test_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Predictions saved to submission.csv!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T06:09:34.911692Z","iopub.execute_input":"2025-02-27T06:09:34.912028Z","iopub.status.idle":"2025-02-27T06:09:58.571243Z","shell.execute_reply.started":"2025-02-27T06:09:34.912002Z","shell.execute_reply":"2025-02-27T06:09:58.570216Z"}},"outputs":[{"name":"stdout","text":"RandomForest F1-Score: 0.6802\nGradientBoosting F1-Score: 0.6842\nLogisticRegression F1-Score: 0.6761\nXGBClassifier F1-Score: 0.7166\nTuning XGBClassifier with GridSearchCV...\nBest XGBClassifier parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\nBest XGBClassifier F1-Score after tuning: 0.7389\n\nPredictions saved to submission.csv!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}